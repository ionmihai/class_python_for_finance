{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lecture Overview\n",
    "\n",
    "The data we need for our projects is rarely all in one place (dataset) or organized the way we need it. This means that we very often have to combine two or more datasets into a single dataset or change the organization of the dataset to better suit our needs.\n",
    "\n",
    "To keep things easy to visualize, we will use small, fictitious datasets to cover the following topics:\n",
    "\n",
    "- Merging datasets\n",
    "    - By columns (using \"merge\")\n",
    "    - By index (using \"merge\" or \"join\")\n",
    "      \n",
    "- Appending datasets\n",
    "\n",
    "- Concatenating datasets\n",
    "        \n",
    "- Reshaping datasets\n",
    "    - From long to wide\n",
    "    - From wide to long\n",
    "       \n",
    "- Sorting datasets\n",
    "    - By index\n",
    "    - By values\n",
    "    \n",
    "    \n",
    "We finish with an application where we use the above techniques to merge the CRSP and Compustat datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Pretty print all cell's output and not just the last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Merging datasets\n",
    "\n",
    "When we say we want to \"merge\" two datasets, we generally mean that we want the columns of the two datasets to appear side by side in one final dataset. The important question is: How should the ROWS of the two datasets be matched? To perform this match, we need to have one or more columns that contain the same information in each of the two datasets. These common columns are ususally referred to as the \"keys\" on which the rows are matched. \n",
    "\n",
    "The second thing we have to decide is what to do with the rows that do NOT match after the merge. This is where we have to decide if we want an \"inner\", \"outer\", \"left\", or \"right\" join (aka merge), as specified below.\n",
    "\n",
    "Finally, and this is specific to Pandas: if the keys are indexes in the dataframes we want to merge, then we need to be carefull to specify that when we use the \"merge\" function. Alternatively, we can use the \"join\" function. This case is covered in section 3.2. below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example datasets\n",
    "df1 = pd.DataFrame({'year': [2001, 2002, 2003], \n",
    "                    'tic': ['MSFT','TSLA','AAPL'], \n",
    "                    'fy':[2002,2003,2004]})\n",
    "df1\n",
    "df2 = pd.DataFrame({'years': [2001, 2002, 2004], \n",
    "                    'ticker': ['MSFT','NFLX','AAPL'], \n",
    "                    'fy':[12,12,12]})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Merging by columns\n",
    "\n",
    "In this section, we assume the keys on which you want to merge are columns (not indexes) in the two dataframes. The keys are specifed with the \"left_on\" and \"right_on\" arguments to the merge function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Inner join\n",
    "\n",
    "The inner join combines the datasets based on the INTERSECTION of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on year data\n",
    "inner1 = df1.merge(df2, how='inner', \n",
    "                   left_on='year', right_on='years') \n",
    "inner1\n",
    "    # Specify how to change names of common variables (fy)\n",
    "inner2 = df1.merge(df2, how='inner', \n",
    "                   left_on='year', right_on='years', \n",
    "                   suffixes=('_df1','_df2'))\n",
    "inner2\n",
    "\n",
    "# Merge on ticker data\n",
    "inner3 = df1.merge(df2, how='inner', \n",
    "                   left_on='tic', right_on='ticker')\n",
    "inner3\n",
    "\n",
    "# Merge on year and ticker\n",
    "inner4 = df1.merge(df2, how = \"inner\", \n",
    "                   left_on = ['year','tic'], \n",
    "                   right_on = ['years','ticker'], \n",
    "                   suffixes=('_df1','_df2'))\n",
    "inner4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Outer join\n",
    "\n",
    "The outer join combines the datasets based on the UNION of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on ticker data\n",
    "outer = df1.merge(df2, how='outer', \n",
    "                  left_on='tic', right_on='ticker')\n",
    "outer\n",
    "\n",
    "#Note how year data has been converted to float because of the introduction of NaN values (which are float)\n",
    "outer.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Left join\n",
    "\n",
    "In a left join, the unmatched keys from the left dataset are kept, but the unmatched keys from the right dataset are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on ticker data\n",
    "left = df1.merge(df2, how='left', \n",
    "                 left_on='tic', right_on='ticker')\n",
    "left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. Right join\n",
    "\n",
    "In a right join, the unmatched keys from the right dataset are kept, but the unmatched keys from the left dataset are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on ticker data\n",
    "right = df1.merge(df2, how='right', \n",
    "                  left_on='tic', right_on='ticker')\n",
    "right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Merging on index\n",
    "\n",
    "As mentioned above, this covers the situation when the keys on which we want to perform the merge are indexes in the dataframes we want to merge. In this case, we can either use the \"merge\" function and specify \"left_index=True, right_index=True\", or we can use the \"join\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add indices to the example dataframes\n",
    "df3 = df1.set_index(['year','tic'])\n",
    "df3\n",
    "df4 = df2.set_index(['years','ticker'])\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using merge function (perform outer merge)\n",
    "#outerm = df3.merge(df4, how='outer', left_index=True, right_index=True) #gives error\n",
    "\n",
    "#Make index names match\n",
    "df4.index.names = ['year','tic']\n",
    "df3\n",
    "df4\n",
    "\n",
    "# Try the merge again\n",
    "outerm = df3.merge(df4, how='outer', \n",
    "                   left_index=True, right_index=True, \n",
    "                   suffixes=('_df3','_df4')) \n",
    "outerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using join function\n",
    "#outerj = df3.join(df4, how = 'outer') #gives error\n",
    "outerj = df3.join(df4, how = 'outer', \n",
    "                  lsuffix='_d3', rsuffix='_d4')\n",
    "outerj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Appending datasets\n",
    "\n",
    "When we say that we want to append a dataset to another, we generally mean that we want the columns of the two datasets to be stacked on top of each other (as opposed to side by side, for the merge). This usually happens when we obtain more data on a given set of variables, and we just want to add it at the bottom of a dataset containing those variables (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append df2 to df1\n",
    "df1\n",
    "df2\n",
    "df1.append(df2)\n",
    "df1 \n",
    "#this shows that df1 has not actually been re-written \n",
    "#(for that we would have to say df1 = df1.append(df2) above)\n",
    "\n",
    "# Rename columns so Python knows which columns contain the same type of information in the two datasets\n",
    "# \"ignore_index\" makes sure the column numbering does not start from 0 again\n",
    "df5 = df1.append(df2.rename(columns={'years':'year',\n",
    "                                     'ticker':'tic', \n",
    "                                     'fy':'fym'}), \n",
    "                 ignore_index=True) \n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Concatenating datasets\n",
    "\n",
    "Usually when we say we want to concatenate datasets, we mean that we want to combine them side by side (just like merge), but \"as they are\", without concern for matching rows in any meaningful way (like merge). The \"concat\" function does this for us (using axis = 1), though it can can also append datasets (using axis = 0, or just leaving out axis altogether)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate df2 to df1\n",
    "df1\n",
    "df2\n",
    "df6 = pd.concat([df1, df2], axis=1) #without axis=1, this is just \"append\"\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Reshaping datasets\n",
    "\n",
    "By reshaping a dataset we generally mean that we want to change the structure of the dataset so that either\n",
    "\n",
    "1. Some data stored in one column is converted to multiple columns (but the same row)\n",
    "    - In pandas, this is called \"unstacking\"\n",
    "    - Informally, we say that we are converting the dataset from long to wide\n",
    "    \n",
    "\n",
    "1. Some data stored in multiple columns (but the same row) is converted to a single column\n",
    "    - In pandas, this is called \"stacking\"\n",
    "    - Informally, we say that we are converting the dataset from wide to long\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 From long to wide (unstacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example dataset\n",
    "long1 = pd.DataFrame({'p':[1,1,2,2], \n",
    "                      'time': ['2005','2006','2005','2006'], \n",
    "                      'ret': [0.1,0.15,0.05,0.01],\n",
    "                      'n':[5,3,4,10]})\n",
    "long1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to wide, where each \"p\" gets its own column and each \"time\" gets its own row\n",
    "\n",
    "    # First we have to set the index\n",
    "long2 = long1.set_index(['time','p'])\n",
    "long2\n",
    "\n",
    "    # Now reshape to wide\n",
    "wide = long2.unstack(level='p')\n",
    "wide\n",
    "wide.columns\n",
    "wide.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. From wide to long (stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack \"wide\" back to \"long\"\n",
    "long3 = wide.stack(level='p')\n",
    "long3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sorting datasets\n",
    "\n",
    "Sorting is a straightforward concept. In Pandas, we use the \"sort_index\" function to sort the dataframe by the index (assumes an index is set), and \"sort_values\" to sort the dataframe by a variable (or a set of variables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by the index\n",
    "long3.sort_index(level='p')\n",
    "long3.sort_index() #this sorts by the first dimension of the multi-level index (time in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by values (columns)\n",
    "long3.sort_values('n')\n",
    "long3.sort_values('n', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Application\n",
    "\n",
    "Perform an inner join on CRSP and Commpustat datasets based on date and permno. \n",
    "\n",
    "You will have to:\n",
    "- Create a monthly frequency date variable (mdate) in each dataset\n",
    "- Set the index in each dataset (permno mdate)\n",
    "- Make sure the index names match in both datasets\n",
    "- Merge the datasets by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crspm (just date, permno and ret)\n",
    "crsp = pd.read_csv('./crspm.zip', sep = '\\t', \n",
    "                   usecols = ['date', 'PERMNO', 'RET'], \n",
    "                   low_memory = False)\n",
    "\n",
    "# Convert column names to lowercase\n",
    "crsp.columns = crsp.columns.str.lower()\n",
    "\n",
    "# Create dtdate (datetime variable) and mdate (month end frequency date)\n",
    "crsp['dtdate'] = pd.to_datetime(crsp['date'], \n",
    "                                format='%Y%m%d') \n",
    "crsp['mdate'] = crsp['dtdate'].dt.to_period('M')\n",
    "\n",
    "# Set index\n",
    "crsp.set_index(['permno', 'mdate'], inplace = True)\n",
    "crsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load compa dataset (just lpermno, datadate, at)\n",
    "comp = pd.read_csv('./compa.zip', sep ='\\t', \n",
    "                   usecols = ['LPERMNO', 'datadate','at'], low_memory = True)\n",
    "\n",
    "# Rename LPERMNO to permno\n",
    "comp.rename(columns = {'LPERMNO':'permno'}, inplace = True)\n",
    "\n",
    "# Create dtdate (datetime variable) and mdate (month end frequency date)\n",
    "comp['dtdate'] = pd.to_datetime(comp['datadate'], \n",
    "                                format='%Y%m%d') \n",
    "comp['mdate']  = comp['dtdate'].dt.to_period('M')\n",
    "\n",
    "# Set index\n",
    "comp.set_index(['permno', 'mdate'], \n",
    "               inplace = True, drop = False)\n",
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge crsp on comp on index (inner)\n",
    "inner = comp.join(crsp, how='inner', rsuffix='_crsp')\n",
    "inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is a difference between the daily dates in comp and crsp\n",
    "inner['dif'] = inner['dtdate'] - inner['dtdate_crsp']\n",
    "inner['dif'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Resources\n",
    "\n",
    "- general discussion on combining datasets\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "\n",
    "\n",
    "- merge function\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html\n",
    "\n",
    "\n",
    "- join function\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html\n",
    "    \n",
    "    \n",
    "- append function\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html\n",
    "\n",
    "\n",
    "- concat function\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html#pandas.concat\n",
    "\n",
    "\n",
    "- general discussion on reshaping and pivot tables\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html\n",
    "\n",
    "\n",
    "- reshaping functions\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html#pandas.DataFrame.stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
