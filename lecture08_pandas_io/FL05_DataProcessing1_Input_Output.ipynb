{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lecture Overview\n",
    "\n",
    "\n",
    "\n",
    "- Downloading data using packages\n",
    "    - yfinance\n",
    "    - datareader\n",
    "    \n",
    "    \n",
    "- Saving and loading data to/from various file formats\n",
    "    - delimited (txt, csv)\n",
    "        - as applications, we open the \"compa\" and \"crspm\" files that we will be using extensively over the entire course\n",
    "    - MS Excel (xlsx)\n",
    "        - additional package required: openpyxl (or xlsxwriter)\n",
    "    - proprietary: Python (pkl), SAS (sas7bdat), Stata (dta), Matlab (mat), etc\n",
    "        - we only showcase Python and Stata \n",
    "        \n",
    "        \n",
    "- Big data solutions\n",
    "    - reading (and processing) datasets in chunks\n",
    "    - using HFD5 files for better performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Downloading data using python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Preliminaries\n",
    "\n",
    "We first need to install the \"pandas_datareader\" package by typing \"pip install pandas_datareader\" in the Anaconda Prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pandas_datareader as pdr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. The DataReader package\n",
    "\n",
    "The **pandas-datareader** package can be used to obtain data from many different sources. See the link in the Resources section below for a documentation of all its capabilities.\n",
    "\n",
    "In this lecture we will only use it to get data on the CPI from the St. Louis FRED database, and data on the 1-month Tbill from Ken French's database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Downloading macro data from St Louis Fred\n",
    "\n",
    "- From the St. Louis FRED, download monthly data on the CPI for all urban consumers (CPIAUCSL). \n",
    "    - Plot the CPI\n",
    "    - Calculate the percentage change in the CPI each month, to obtain the inflation rate\n",
    "    - Plot the inflation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CPI data from FRED\n",
    "cpi = pdr.DataReader('CPIAUCSL','fred','1970-01-01','2020-12-31')\n",
    "print(cpi)\n",
    "cpi.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inflation rate\n",
    "infl = cpi.pct_change()\n",
    "print(infl)\n",
    "infl.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Downloading data from the Ken French database\n",
    "\n",
    "- From the Ken French database, download monthly data on the risk-free rate (1-month tbill rate).\n",
    "    - Use the \"pandas-datareader\" package\n",
    "    - This is the \"RF\" column in the \"F-F_Research_Data_Factors\" database\n",
    "    - The rate is expressed in percentage points so you will have to divide it by 100\n",
    "    - Plot the resulting risk-free rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the names of all the available datasets from Ken French database\n",
    "pdr.famafrench.get_available_datasets()[0:9] #print just the top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the monthly Fama French factors (first item in the list)\n",
    "ff3f = pdr.DataReader('F-F_Research_Data_Factors', 'famafrench','1970-01-01')\n",
    "ff3f\n",
    "print(ff3f['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the monthly table\n",
    "monthly_dat = ff3f[0]\n",
    "monthly_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the monthly risk-free rate\n",
    "rfdat = monthly_dat['RF']\n",
    "rfdat = rfdat/100\n",
    "print(rfdat)\n",
    "rfdat.plot();\n",
    "print(rfdat.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. The yfinance package\n",
    "\n",
    "We already used this package in the previous lecture, and, going forward, we will not use much more of its functionality other than downloading monthly data on a few tickers (as below). Please see the link in the Resources section below to explore more of its capabilities.\n",
    "\n",
    "One important thing to keep in mind about the Yahoo Finance data, is that, for individual stocks, you should use \"Adj Close\" prices to calculate returns (like in the previous lecture), but for indexes, you should use the \"Close\" values to calculate returns (as below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application: Performance of main asset classes**\n",
    "\n",
    "- From Yahoo Finance, download monthly data on the SPDR S&P 500 ETF, the SPRD Gold Shares ETF, and BlackRock's long-term (20+ years) treasury ETF (tickers: SPY, GLD, TLT respectively). \n",
    "    - Download data from 2004 to 2020\n",
    "    - Convert these data to monthly returns\n",
    "    - Calculate the returns on an equal-weighted portfolio of the 3 asset classes\n",
    "    - Plot all the montly returns on the same graph\n",
    "    - Calculate rolling compounded returns \n",
    "    - Plot the rolling compounded returns on the same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Yahoo Finance data\n",
    "yfdat = yf.download(tickers = ['SPY', 'GLD', 'TLT'], \n",
    "                    start = '2002-01-01', end = '2020-12-31',\n",
    "                    interval = '1mo')\n",
    "\n",
    "# Always inspect the structure of your data when you are not familiar with the dataset\n",
    "print(yfdat.index,'\\n\\n')    # These are like the line numbers in an excel sheet but much more general\n",
    "print(yfdat.columns, '\\n\\n') # These are column names, but they can have multiple parts\n",
    "print(yfdat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the 'Close' values and drop missing values\n",
    "yfdat = yfdat['Close'].dropna()\n",
    "yfdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolution of asset classes during the sample\n",
    "yfdat.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate monthly returns\n",
    "yfret = yfdat.pct_change()\n",
    "\n",
    "# Add column with returns of equal-weighted portfolio\n",
    "yfret['EW_Portf'] = yfret.mean(axis = 1)\n",
    "\n",
    "# Plot rolling compounded returns\n",
    "(1+yfret).cumprod().plot(); \n",
    "    #note that Python will evaluate the above expression from left to right\n",
    "    #so we can chain instructions on the same line of code in the order that we want them executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saving and loading data from various file formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Reading and writing .csv and .txt files with \"read_csv\" and \"to_csv\"\n",
    "\n",
    "The WRDS data files under the Datasets tab in D2L (\"crspm\" and \"compa\") were saved as tab-delimited txt files. The \"read_csv\" pandas function can read basically every type of delimited file, as long as you specify what the delimiter is (comma, space, tab), as below.\n",
    "\n",
    "See the \"read_csv\" link in the \"Resources\" section below for a description of the full functionality of \"read_csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application: Saving and loading Pandas dataframes using .csv files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the \"yfdat\" dataset to a csv file\n",
    "yfdat.to_csv(\"./L05_yfdat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the csv file we create above \n",
    "yfdat_load1 = pd.read_csv(\"./L05_yfdat.csv\")\n",
    "yfdat_load1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the save file, this time specifying header and index\n",
    "yfdat_load2 = pd.read_csv(\"./L05_yfdat.csv\",\n",
    "                         index_col = [0],\n",
    "                         header = [0])\n",
    "yfdat_load2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application: Loading the CRSPM (.txt) file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CRSP data from WRDS (crspm file)\n",
    "crsp = pd.read_csv(\"./crspm.zip\",   # the \"./\" means the crspm file is in the same folder as these lecture notes\n",
    "                   sep = '\\t',      # specifies that the crspm file is tab delimited\n",
    "                   usecols = ['PERMNO', 'date', 'RET'],  #allows us to select only a subset of all the columns\n",
    "                   low_memory = False) # specifies that we want to read the whole file in one chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the dataset\n",
    "print(crsp, '\\n\\n')\n",
    "print(crsp.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application: Loading the COMPA (.zip) file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Compustat annual files (compa) specifying that it is zipped\n",
    "comp = pd.read_csv(\"./compa.zip\",   # the \".gz\" means this file was archived with gzip \n",
    "                   sep = '\\t',         \n",
    "                   usecols = ['LPERMNO', 'datadate', 'at'],  \n",
    "                   low_memory = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the dataset\n",
    "print(comp, '\\n\\n')\n",
    "print(comp.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Reading and writing Excel files\n",
    "\n",
    "To read and write Excel files, we need two more packages: xlrd (to read Excel files) and openpyxl (to write excel files). \n",
    "\n",
    "To install these packages, open the Anaconda Prompt (or a terminal) and type:\n",
    "\n",
    "conda install -y openpyxl xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the yfdat data to excel\n",
    "yfdat.to_excel(\"./L05_yfdat.xlsx\",\n",
    "              sheet_name = \"Asset_Classes\",\n",
    "              startrow = 2,\n",
    "              startcol = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from excel file we created above (to showcase what may go wrong)\n",
    "yfdat_excel1 = pd.read_excel(\"./L05_yfdat.xlsx\") #, index_col = [0])\n",
    "yfdat_excel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read it again, this time specifying where the index and header are (and more functionality)\n",
    "yfdat_excel2 = pd.read_excel(\"./L05_yfdat.xlsx\",\n",
    "                            sheet_name = \"Asset_Classes\",\n",
    "                            usecols = \"F:I\", \n",
    "                            skiprows = [0,1],\n",
    "                            nrows = 4, \n",
    "                            index_col = [0],\n",
    "                            header = [0])\n",
    "yfdat_excel2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Reading and writing from/to common proprietary file formats\n",
    "\n",
    "Pandas has several functions that allow us to read and write data from many different types of files. Several of these are files that are created using expensive software like SAS, Stata, and Matlab.\n",
    "\n",
    "Below, we only showcase reading a Stata file (the \"comp.dta\" file found in the Datasets tab in D2L). This is done with the \"read_stata\" function. Writing Stata data files can be done with the \"to_stata\" function. We do not show this here because it's likely none of you have Stata. \n",
    "\n",
    "The point of this section is to make you aware that, if, for some reason you come across datasets that are in proprietary file formats, Python (and Pandas in particular) will likely have a way to allow you to read that dataset, even if you don't have the software that was used to create it in the first place. \n",
    "\n",
    "Please visit the \"I/O tools\" link in the Resources section below for additional information on how to read or write these additional file formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Compustat annual file saved in Stata format (compa.dta)\n",
    "#comp_stata = pd.read_stata(\"./compa.dta\",\n",
    "#                          columns = ['LPERMNO', 'datadate', 'at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the dataset\n",
    "#print(comp_stata.head(5), '\\n\\n')\n",
    "#print(comp_stata.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More importantly for this course, Python has a proprietary data format called \"pickle\". Saving and loading data from pickle (.pkl extension) files is significantly faster than from/to csv, so we will be using it quite a bit later on in the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a dataframe to a pkl file\n",
    "comp.to_pickle('./comp.pkl')\n",
    "\n",
    "# Read a dataframe from a pkl file\n",
    "comp_pkl = pd.read_pickle('./comp.pkl')\n",
    "comp_pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. \"Big Data\" solutions: working with very large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Reading and processing large files in \"chunks\"\n",
    "\n",
    "Show how to use a for loop to (1) read a file in chunks and (2) apply some simple processing to each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the COMPA file, 10,000 rows at a time and retain the firm with most total assets (AT)\n",
    "oldmax = 0;\n",
    "for chunk in pd.read_csv('./compa.zip', sep='\\t', \n",
    "                         chunksize=10000, \n",
    "                         usecols=['LPERMNO','datadate','at', 'tic']):\n",
    "    newmax = chunk['at'].max()\n",
    "    if newmax > oldmax:\n",
    "        comp_info = chunk.loc[chunk['at']==newmax,:].copy()\n",
    "        oldmax = newmax\n",
    "        print(comp_info, end='\\n\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Using HDF5 files for better performance\n",
    "\n",
    "To use some of the functionality of HDF files, we need the \"tables\" package. Install it by typing the following in the Anaconda Prompt (or a terminal):\n",
    "\n",
    "pip install tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read COMPA file and time it\n",
    "import time\n",
    "tic = time.perf_counter()\n",
    "comp = pd.read_csv(\"./compa.zip\",  sep = '\\t', \n",
    "                   usecols=['LPERMNO','datadate','at'])\n",
    "toc = time.perf_counter()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to HDF format\n",
    "comp.to_hdf('./comp.hdf', key='comp',\n",
    "            data_columns = ['LPERMNO','datadate','at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read HDF file and time it\n",
    "tic = time.perf_counter()\n",
    "comp_hdf = pd.read_hdf('./comp.hdf', key='comp')\n",
    "toc = time.perf_counter()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexing HDF files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an HDF file with the \"table\" format so we can index it later on\n",
    "comp.to_hdf('./comp_at.h5', key='comp_at', \n",
    "                  format='table',\n",
    "                  data_columns = ['LPERMNO','datadate','at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve only the data that satisfies some condition (keep only data from december 2018)\n",
    "comp_large = pd.read_hdf('./comp_at.h5', key='comp_at', \n",
    "                         where = [\"datadate == 20181231\"])\n",
    "comp_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Resources\n",
    "\n",
    "- I/O tools for Pandas data:\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
