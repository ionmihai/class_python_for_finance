{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884f2970",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preliminaries\" data-toc-modified-id=\"Preliminaries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preliminaries</a></span></li><li><span><a href=\"#Relative-vs-absolute-paths\" data-toc-modified-id=\"Relative-vs-absolute-paths-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Relative vs absolute paths</a></span></li><li><span><a href=\"#Reading-and-writing-.pkl-(pickle)-files\" data-toc-modified-id=\"Reading-and-writing-.pkl-(pickle)-files-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Reading and writing .pkl (pickle) files</a></span><ul class=\"toc-item\"><li><span><a href=\"#.to_pickle()\" data-toc-modified-id=\".to_pickle()-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span><code>.to_pickle()</code></a></span></li><li><span><a href=\"#.read_pickle()\" data-toc-modified-id=\".read_pickle()-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span><code>.read_pickle()</code></a></span></li></ul></li><li><span><a href=\"#Reading-and-writing-.txt-and-.csv-files\" data-toc-modified-id=\"Reading-and-writing-.txt-and-.csv-files-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Reading and writing .txt and .csv files</a></span><ul class=\"toc-item\"><li><span><a href=\"#.to_csv()\" data-toc-modified-id=\".to_csv()-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span><code>.to_csv()</code></a></span></li><li><span><a href=\"#.read_csv()\" data-toc-modified-id=\".read_csv()-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span><code>.read_csv()</code></a></span></li></ul></li><li><span><a href=\"#Reading-and-writing-.xslx-files\" data-toc-modified-id=\"Reading-and-writing-.xslx-files-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Reading and writing .xslx files</a></span><ul class=\"toc-item\"><li><span><a href=\"#.to_excel()\" data-toc-modified-id=\".to_excel()-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span><code>.to_excel()</code></a></span></li><li><span><a href=\"#.read_excel()\" data-toc-modified-id=\".read_excel()-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span><code>.read_excel()</code></a></span></li></ul></li><li><span><a href=\"#Some-important-data-acquisition-packages\" data-toc-modified-id=\"Some-important-data-acquisition-packages-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Some important data-acquisition packages</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-requests-package\" data-toc-modified-id=\"The-requests-package-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>The <code>requests</code> package</a></span></li><li><span><a href=\"#The-pandas_datareader-package\" data-toc-modified-id=\"The-pandas_datareader-package-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>The <code>pandas_datareader</code> package</a></span></li><li><span><a href=\"#The-yfinance-package\" data-toc-modified-id=\"The-yfinance-package-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>The <code>yfinance</code> package</a></span></li><li><span><a href=\"#The-wrds-package-(OPTIONAL)\" data-toc-modified-id=\"The-wrds-package-(OPTIONAL)-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>The <code>wrds</code> package (OPTIONAL)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81dffb",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "If you have not done so already (you were asked to do this in ``lecture01``):\n",
    "\n",
    "\n",
    "1. Open a Terminal, type the following command and hit enter:\n",
    "    - pip install yfinance pandas-datareader\n",
    "    \n",
    "    \n",
    "2. Open a Terminal, type the following command and hit enter:\n",
    "    - conda install -y openpyxl xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pandas_datareader as pdr\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6a246",
   "metadata": {},
   "source": [
    "# Relative vs absolute paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de24a1",
   "metadata": {},
   "source": [
    "In this lecture, we will have to tell Python exactly where on our drive it should store some files we create. We can do this by specifying the full path to these locations, for example, on my computer:\n",
    "\n",
    "'C:/Users/ionmi/Dropbox/TEACHING'\n",
    "\n",
    "The above is an **absolute path**: it contains the full path to the TEACHING folder on my drive. \n",
    "\n",
    "An alternative way to specify a path (which we will use very often in this course) is to specify that path **relative** to the current working directory. To do this, we use a combination of one or more dots ('.') and/or slashes ('/') which have the following meaning:\n",
    "\n",
    "'.' means the current working directory (in our case, this is the directory where these lecture notes are stored on the drive).\n",
    "\n",
    "'..' means the parent directory of the current working directory.\n",
    "\n",
    "'../..' means the parent of the parent of the current working directory.\n",
    "\n",
    "'../../..' means the parent of the parent of the parent of the current working directory.\n",
    "\n",
    "etc.\n",
    "\n",
    "To see this in practice, we can use the ``Path`` function in the ``pathlib`` package (imported above), which allows us to see the absolute path of a given relative path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34848b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ce5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d289b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d2e99d5",
   "metadata": {},
   "source": [
    "# Reading and writing .pkl (pickle) files\n",
    "\n",
    "Python has a proprietary data format called \"pickle\". These types of files have the extension \".pkl\". Saving and loading data from pickle files is significantly faster than from/to \".csv\", so we will be using it quite a bit throughout the course (note, however, the important security warning at the top of the official documentation: https://docs.python.org/3/library/pickle.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cea14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "091c3056",
   "metadata": {},
   "source": [
    "## ``.to_pickle()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637cd48",
   "metadata": {},
   "source": [
    "To store this data in a .pkl file, we use the \"``.to_pickle``\" function, applied right after the name of the dataframe which contains the data we want to store.\n",
    "\n",
    "\n",
    "Syntax:\n",
    "\n",
    "```python\n",
    "DataFrame.to_pickle(path, compression='infer', protocol=5, storage_options=None)\n",
    "```\n",
    "\n",
    "Note that the first argument (``path``) is mandatory (it has no default value). This argument is where you specify the **name** of the .pkl file you want to create (``mydata.pkl`` below) and the **location** (directory) where this file should be stored (``.`` below) all in a single string, separated by ``/``.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bba1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bee68a65",
   "metadata": {},
   "source": [
    "Note that we can also compress the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114e161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5074af6",
   "metadata": {},
   "source": [
    "## ``.read_pickle()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139547b9",
   "metadata": {},
   "source": [
    "To read data from an existing .pkl file, we use the \"``.read_pickle``\" function, specifying as an argument the path to the file we want to read (including its name):\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "pandas.read_pickle(filepath_or_buffer, compression='infer', storage_options=None)\n",
    "```\n",
    "\n",
    "For example, if we want to read the contents of the .pkl file we just created above, and store those contents into a new variable ``df2``, we would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf44540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771b3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1716d89d",
   "metadata": {},
   "source": [
    "And we can read compressed .pkl files too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d5fb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171e8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acde568d",
   "metadata": {},
   "source": [
    "Note a very important difference in how we use the two functions above. The syntax for ``.to_pickle()`` starts with ``DataFrame.to_pickle`` which tells us that the function must be applied to an existing dataframe. On the other hand, the syntax for ``.read_pickle()`` starts with ``pandas.read_pickle``, which we converted to ``pd.read_pickle``, because we imported pandas as pd in the first cell code in this notebook (at the top). \n",
    "\n",
    "This pattern is the same for all the read-write functions we discuss in this lecture: the \"write\" functions (``.to_pickle()``, ``.to_csv()``, ``.to_excel()``) are written after the name of the dataframe we want to write to a file, while the \"read\" functions (``.read_pickle()``, ``.read_csv()``, ``.read_excel()``) follow the name of the pandas package (which we renamed to ``pd`` above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32882198",
   "metadata": {},
   "source": [
    "# Reading and writing .txt and .csv files \n",
    "\n",
    "The most common way to read and write dataframes from/to .csv and .txt files is with the Pandas functions \"``.to_csv()``\" (for writing) and \"``.read_csv()``\" (for reading):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e71da8",
   "metadata": {},
   "source": [
    "## ``.to_csv()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68846e23",
   "metadata": {},
   "source": [
    "Here is the abbreviated version of the syntax for ``.to_csv()`` excluding parameters that are not used very often:\n",
    "\n",
    "```python\n",
    "DataFrame.to_csv(path_or_buf=None, sep=',', columns=None, header=True, index=True, index_label=None)\n",
    "```\n",
    "\n",
    "Note that the default separator is a comma (``sep=','``) which means we can omit that parameter when we write .csv files. The ``columns`` parameter allows you to specify which columns of the dataframe you want to write in the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ca858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1263fa3d",
   "metadata": {},
   "source": [
    "To write tab-delimited .txt files, we use ``sep='\\t'`` and change the file extension to ``.txt``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d108063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0184220",
   "metadata": {},
   "source": [
    "To write space-delimited .txt files, we use ``sep=' '`` (though I always recommend using tabs for .txt files):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1cb6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fda03f2",
   "metadata": {},
   "source": [
    "## ``.read_csv()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263db650",
   "metadata": {},
   "source": [
    "Here is the abbreviated version of the syntax for ``.read_csv()`` excluding parameters that are not used very often:\n",
    "\n",
    "```python\n",
    "pandas.read_csv(filepath_or_buffer, sep=',',  header='infer', names=None, index_col=None, usecols=None, nrows=None, \n",
    "                skiprows = None)\n",
    "```\n",
    "\n",
    "Again, the default separator is a comma (``sep=','``) which means we can omit that parameter when we read .csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd665d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e634106",
   "metadata": {},
   "source": [
    "Note however, that we did not specify that the first column is just an index for the table, so that first column was included as data in the table itself. Note also that ``.read_csv()`` guessed that the column names are on the first row, because the default value of the ``header`` parameter is ``infer``. To be safe, I always recommend being explicit about where the row names and column names are (and remember that Python starts counting from 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd71b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d640af8f",
   "metadata": {},
   "source": [
    "To read tab-delimited .txt files, we use ``sep='\\t'``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a6bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "974c2bfe",
   "metadata": {},
   "source": [
    "To read space-delimited .txt files, we use ``sep=' '``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ca67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78752c06",
   "metadata": {},
   "source": [
    "The ``.to_csv()`` and ``.read_csv()`` functions have several other useful parameters. In the practice problems for this lecture, you will be asked to investigate some of them on your own by reading the official documentation:\n",
    "\n",
    "- ``.to_csv()``: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "- ``.read_csv()``: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ea4b8",
   "metadata": {},
   "source": [
    "# Reading and writing .xslx files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f42cbf",
   "metadata": {},
   "source": [
    "We'll use ``.to_excel()`` to write Excel files and ``.read_excel()`` to read Excel files. The biggest difference from the .csv functions is that, with the excel ones, you can specify a particular sheet in the the Excel file that you want to read/write."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51613dde",
   "metadata": {},
   "source": [
    "## ``.to_excel()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8b101",
   "metadata": {},
   "source": [
    "Here is an abbreviate version of the syntax for ``.to_excel()``:\n",
    "\n",
    "```python\n",
    "DataFrame.to_excel(excel_writer, sheet_name='Sheet1', columns=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aeb802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbe9ad53",
   "metadata": {},
   "source": [
    "## ``.read_excel()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34d8ff",
   "metadata": {},
   "source": [
    "Here is an abbreviated version of the syntax for ``.read_excel()``:\n",
    "\n",
    "```python\n",
    "pandas.read_excel(io, sheet_name=0, header=0, names=None, index_col=None, usecols=None, skiprows=None, nrows=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c444b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "043b2949",
   "metadata": {},
   "source": [
    "The ``.to_excel()`` and ``.read_excel()`` functions have several other useful parameters. In the practice problems for this lecture, you will be asked to investigate some of them on your own by reading the official documentation:\n",
    "\n",
    "- ``.to_excel()``: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html\n",
    "- ``.read_excel()``: https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3882f7f7",
   "metadata": {},
   "source": [
    "# Some important data-acquisition packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9adde76",
   "metadata": {},
   "source": [
    "## The ``requests`` package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478afee8",
   "metadata": {},
   "source": [
    "The ``requests`` package allows us to retrieve data from websites. If you want a more detailed discussion of the full functionality of this package, see the documentation at https://docs.python-requests.org/en/latest/. \n",
    "\n",
    "Here, we'll just see how we can use the package to download data from files hosted on websites. For this, we need the URL to the file we want to download. In the example below, I use data on economic policy uncertainty in the US from this website:\n",
    "\n",
    "https://www.policyuncertainty.com/us_monthly.html\n",
    "\n",
    "If you right-click on the \"Download Data\" link and select \"Copy Link Address\", you should see the link below when you paste it in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b127e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18e5171d",
   "metadata": {},
   "source": [
    "We use the ``.get()`` function to retrieve the (binary) data from the URL above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3104a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d70d9a7",
   "metadata": {},
   "source": [
    "We can check if the request was successful using the ``status_code`` attribute. 200 means the request was successful, 404 means there was an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e457a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d8c4651",
   "metadata": {},
   "source": [
    "To write the data we retrieved into an Excel file on our computer, we use the Python built-in ``open()`` function, specifying the path to the file we want to write the data to (``'./policy_uncertainty.xlsx`` below), specifying that we are writing binary data in it (``wb`` below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59a1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fd9e5d5",
   "metadata": {},
   "source": [
    "This just opens the './policy_uncertainty.xlsx' file (creates it if it doesn't exist) and returns something like the address to that file on the drive. It does not write anything in that file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def85fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bb9b876",
   "metadata": {},
   "source": [
    "Now we can write the data into that file using the Python built-in ``write()`` function and then closing that file with the ``close()`` function. Note that the actual data from the URL above is found under the ``content`` attribute of the request ``r`` that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d995c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee1f6665",
   "metadata": {},
   "source": [
    "We can check if this process worked by either manually opening the ``policy_uncertainty.xlsx`` in our working directory, or by using ``pd.read_excel()`` to just read the data into a dataframe and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4d3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38111f87",
   "metadata": {},
   "source": [
    "## The ``pandas_datareader`` package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df5875",
   "metadata": {},
   "source": [
    "The ``pandas_datareader`` package allows us to download data from many different sources on the internet. Here is a list of all these sources:\n",
    "\n",
    "https://pandas-datareader.readthedocs.io/en/latest/readers/index.html\n",
    "\n",
    "The general syntax to download data from a particular source is as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256841a",
   "metadata": {},
   "source": [
    "Syntax:\n",
    "\n",
    "```python\n",
    "pandas_datareader.DataReader(name,data_source=None,start=None,end=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216b6a3",
   "metadata": {},
   "source": [
    "The two sources I will cover here are the St. Louis Federal Reserve Economic Data (FRED) (``data_source = 'fred'``) which contains a lot of useful **macroeconomic data**, and the Fama-French Data (Ken French’s Data Library) (``data_source = 'famafrench'``)  which contains returns on many portfolios commonly used in **asset pricing** (e.g. the market portfolio, SMB, HML, industry portfolios, etc).\n",
    "\n",
    "For both of these sources, we use the ``name`` parameter to specify **what** exactly we want to download from these data sources. \n",
    "\n",
    "For example, to download data on the the CPI from FRED, we need to use ``name = 'CPIAUCSL'`` which is the internal name that FRED uses for the CPI data:\n",
    "\n",
    "https://fred.stlouisfed.org/series/CPIAUCSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33a3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96fbb796",
   "metadata": {},
   "source": [
    "To download data on the Fama-French three risk factors (market, SMB, and HML) we use ``name = 'F_Research_Data_Factors'\"`` which is the name of the text file containing these factors on Ken French's website:\n",
    "\n",
    "https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2df680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf6666ee",
   "metadata": {},
   "source": [
    "Note that for the 'famafrench' data source, the 'DataReader' function return a dictionary of dataframes, not a single pandas dataframe. That's because the 'F-F_Research_Data_Factors' contains multiple tables. The monthly returns on the Fama-French risk factors are in the first entry in that dictionary (the 0 key), so we can retrieve it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3480c929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bb5cc96",
   "metadata": {},
   "source": [
    "There is no easy way to know under what ``name`` you can find the data you need. You have to look at the FRED and Fama-French websites first, to see what names those websites use for the data you need and then type those names into your code, like we did above. The same is true for all the other data sources that pandas_datareader allows us to get data from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5298b7",
   "metadata": {},
   "source": [
    "## The ``yfinance`` package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf573c2",
   "metadata": {},
   "source": [
    "The ``yfinance`` package allows us to retrieve stock price data from Yahoo Finance. The full documentation for the package can be found here: https://pypi.org/project/yfinance/ (in particular, look under \"Fetching data for multiple tickers\" on the main page).\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "yfinance.download(tickers, start = None, end = None, interval = '1d')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5249e02",
   "metadata": {},
   "source": [
    "For example, to retrieve monthly stock prices for Microsoft and Apple, we need to supply their tickers in a single string (separated by a space) as the first parameter to the ``download`` function and change the ``interval`` parameter to ``1mo`` (otherwise it will give us daily data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39a097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea95e637",
   "metadata": {},
   "source": [
    "We will always drop missing values (with ``.dropna()``) and use Adj Close prices (prices adjusted for dividends and splits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584dcba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01abf8e1",
   "metadata": {},
   "source": [
    "Note that, if we download data for a single stock, this will return a pandas Series, not a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d97bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10cb69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b409377",
   "metadata": {},
   "source": [
    "As mentioned before, we will usually turn Series objects into dataframes before continuing to work with them any further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cebef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2172a757",
   "metadata": {},
   "source": [
    "## The ``wrds`` package (OPTIONAL)\n",
    "\n",
    "The ``wrds`` package allows us to download data directly from the WRDS database (Wharton Research Data Services). Unfortunately, this functionality is not available for \"class accounts\" like the ones I created for this course. I only mention this package for PhD students, who should be able to use this package with their own individual WRDS credentials (not the common ones provided for this class). The documentation for this package is found here:\n",
    "\n",
    "https://pypi.org/project/wrds/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
