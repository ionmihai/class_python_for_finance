{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Lecture-overview\" data-toc-modified-id=\"Lecture-overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Lecture overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Application\" data-toc-modified-id=\"Application-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Application</a></span></li></ul></li><li><span><a href=\"#Preliminaries\" data-toc-modified-id=\"Preliminaries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preliminaries</a></span></li><li><span><a href=\"#Descriptive-statistics\" data-toc-modified-id=\"Descriptive-statistics-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Descriptive statistics</a></span></li><li><span><a href=\"#Linear-regression\" data-toc-modified-id=\"Linear-regression-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Linear regression</a></span></li><li><span><a href=\"#Dealing-with-autocorrelated-errors\" data-toc-modified-id=\"Dealing-with-autocorrelated-errors-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Dealing with autocorrelated errors</a></span></li><li><span><a href=\"#Dealing-with-non-stationarity\" data-toc-modified-id=\"Dealing-with-non-stationarity-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Dealing with non-stationarity</a></span><ul class=\"toc-item\"><li><span><a href=\"#Testing-for-stationarity-(unit-root-tests)\" data-toc-modified-id=\"Testing-for-stationarity-(unit-root-tests)-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Testing for stationarity (unit root tests)</a></span></li><li><span><a href=\"#First-differencing\" data-toc-modified-id=\"First-differencing-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>First-differencing</a></span></li><li><span><a href=\"#Controlling-for-deterministic-trends\" data-toc-modified-id=\"Controlling-for-deterministic-trends-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Controlling for deterministic trends</a></span></li><li><span><a href=\"#Eliminating-stochastic-(changing)-trends-(OPTIONAL)\" data-toc-modified-id=\"Eliminating-stochastic-(changing)-trends-(OPTIONAL)-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Eliminating stochastic (changing) trends (OPTIONAL)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Coverage\n",
    "    - We only cover \"static\" models (of the type $y_{t+k} = \\alpha + \\beta \\cdot X_t + \\epsilon_{t+k}$)\n",
    "    - We do not cover dynamic models (e.g. ARIMA models or VAR models which include lags of the dependent variable as explanatory variables)\n",
    "    - We do not cover conditional heteroskedasticity models (e.g. ARCH and GARCH models of the variance of the error term)\n",
    "        \n",
    "\n",
    "- Dealing with autocorrelated errors (failure of assumption A3)\n",
    "    - Newey-West correction\n",
    "\n",
    "\n",
    "- Dealing with non-stationary variables (failure of assumption A2)\n",
    "    - Test for stationarity\n",
    "    - Common ways to address non-stationarity\n",
    "        - First-differencing\n",
    "        - Detrending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "The showcase the tools in this lecture, we will develop a (somewhat crude) test of the Expectations Hypothesis of the term structure of interest rates. In a nutshell, this hypothesis claims long-term rates should equal compounded future expected short-term rates:\n",
    "\n",
    "$$ (1 + r_{t,t+N})^N = (1 + E_t(r_{t,t+1}))(1 + E_t(r_{t+1,t+2}))...(1 + E_t(r_{t+N-1,t+N})) $$\n",
    "\n",
    "Assuming rational expectations, future realized short-term rates should on average match current expectations of those rates. If this is the case, one way we can test the Expectations Hypothesis by testing if current long-term rates can predict future short-term rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this test, we use the yield on 10-year Treasury bonds as our long-term rate, and the yield on the 3-month Treasury bill as our short-term rate. We then regress the 3-month rate from 5 years in the future on the current 10-year rate \n",
    "\n",
    "$$r^{3m}_{t+5} = \\alpha + \\beta \\cdot r^{10yr}_t + \\epsilon_{t+5} $$\n",
    "\n",
    "\n",
    "Yes, the 5-year horizon is quite arbitrary (we should be testing all horizons up to 10 years at the same time), hence my calling it a \"somewhat crude\" test. The purpose of this application is to showcase the common statistical issues one often encounters in time-series regressions. See this paper https://core.ac.uk/download/pdf/6956537.pdf for more thorough tests of the hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading data on the two rates (monthly frequency, not seasonally adjusted) and running the regression mentioned above. The rest of the lecture describes two main issues with this regression (non-stationarity and autocorrelated errors) and describes common tools used to address these issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader as pdr\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.stattools as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download monthly data on 3month and 10yr treasury yields from St Louis FRED\n",
    "rates = pdr.DataReader(['TB3MS','GS10'],'fred','1954-01-01')\n",
    "rates.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "    # Change index to montly period\n",
    "rates.index = rates.index.to_period('M')\n",
    "    # Rename variables\n",
    "rates.rename(columns = {'TB3MS': 'r_3m', 'GS10': 'r_10yr'}, inplace = True)\n",
    "rates    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate future levels of the short rate (m3)\n",
    "rates['r_3m_lead5'] = rates['r_3m'].shift(-60)      #because 60 months = 5 years\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for next time\n",
    "rates.to_pickle('../data/rates.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics\n",
    "\n",
    "We start by just summarising the data and looking at correlations of the variables. With time-series regressions,we want to pay particular attention to how autocorrelated our variables are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always look at autocorrelations in your data before you run a time-series regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are just one-period (1 month) autocorrelations. We can look at all autocorrelations (all lags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "We start by running the simple regression of future short term rates on current long-term rates:\n",
    "\n",
    "\n",
    "$$r^{3m}_{t+5} = \\alpha + \\beta \\cdot r^{10yr}_t + \\epsilon_{t+5} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress future short rate on current long rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low p-value for the ``r_10yr`` coefficient tells us that the long rate is a strong predictor of the future short rate, which is consistent with the Expectations Hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with autocorrelated errors\n",
    "\n",
    "The Durbin-Watson statistic in the regression above is close to 0, which suggests the error terms in our regression are positively autocorrelated. This violates assumption A3 discussed in the regression intro lecture. \n",
    "\n",
    "To address this issue, we can apply the \"get_robustcov_results\" function to the results output from the OLS regression above, specifying that the covariance type (\"cov_type\") needs to be heteroskedasticity-and-autocorrelation consistent (HAC). We also need to specify the maximum number of lags we want to use when correcting our standar errors (the \"maxlag\" parameter). This implements the Newey-West (1987) covariance estimator that adjusts standard errors for autocorrelation in residuals up to the \"maxlag\" you specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the regression without any correction first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most practitioners use $maxlag = N^{1/4}$ where N is the number of observations in our regression (though we will not go into detail about why this value is used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N^(1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Obtain HAC standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this correction does not change the coefficients themselves, just their statistical significance. For example the constant term is no longer statistically significant. More importantly for our application: the long term rate is still a statistically significant predictor of the future short rate, which is consistent with the Expectations Hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with non-stationarity\n",
    "\n",
    "A \"stationary\" time-series process is a process that has a constant mean and variance over time, and autocorrelations depend only on the lag, not on the time period itself (technically speaking, such processes are called \"covariance-stationary\" or \"weakly-stationary\" but these are usually shortened to \"stationary\").\n",
    "\n",
    "If the variables in our time-series regression are non-stationary, the regression estimates are not reliable because non-stationarity can cause:\n",
    "- A \"spurious correlation\" between the two variables (variables may look highly correlated when in fact they are not economically related in any way)\n",
    "    - This is a failure of assumption A2\n",
    "    - This leads to a bias in the regression coefficients (affects conclusions on economic significance)\n",
    "- Heteroskedasticity and autocorrelation in the regression residuals \n",
    "    - This is a failure of assumption A3\n",
    "    - In thise case, t-statistics and p-values will be miscalculated (affects conclusions on statistical significance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for stationarity (unit root tests)\n",
    "\n",
    "Before you run ANY time series regression, you need to first test if the variables in your regression are stationary (such tests are commonly referred to as \"unit root tests\"). There are several test you could use for this purpose, but one of the most common ones is the Augmented Dickey-Fuller test (ADF). You can implement this test using the \"adfuller\" function from the \"statsmodels.tsa.stattools\" package (\"st\" below).\n",
    "\n",
    "The null hypothesis in the ADF test is that the series is non-stationary. So if the test returns a small p-value (e.g smaller that 1%), we can conclude that the series does not suffer from non-stationarity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if short rate is not stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value above is larger than 1\\% so we can not conclude that the short rate is non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if long rate is not stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value above is larger than 1\\% so we can not conclude that the long rate is non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-differencing\n",
    "\n",
    "The most commonly used method to convert a non-stationary series into a stationary series it to first-difference it (i.e. current level minus the previous level of the series). Technically, this assumes that the non-stationary series has \"order of integration 1\" which is the case for most economic series of interest. You don't need to understand what that means for this class. I am only mentioning it so you understand that sometimes, first-differencing may NOT produce a stationary series. In such cases, a second-difference may help: take a first-difference of the first-difference. Alternatively, use the detrending methods in sections 3.3. and 3.4. below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate first diferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the differenced series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at autocorrelation of differenced short rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the 1-month autocorrelation is still quite high. But, if we look at the autocorrelation plot (the gray lines are 95\\% confidence intervals), most of these autocorrelations are statistically insignificant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to formally test if we still have a non-stationary problem, we run a ADF test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if differenced short rate is non-stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is virtually 0, so we can reject the null of non-stationarity for the first-differenced short rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if differenced long rate is non-stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the p-value is virtually 0, so we can reject the null of non-stationarity for the first-differenced long rate too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run regression using differenced variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain HAC standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results differ from the non-differenced regression in a very crucial way: the p-value of the ``r_10yr_change`` variable is not lower than 1\\% anymore, so we can not reject the null that the long rate has no predictive power over the short rate. This **contradicts** the prediction of the Expectations Hypothesis that long rates should have statistically significant predictive power over short rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling for deterministic trends\n",
    "\n",
    "If we believe that our variables are non-stationary because of a deterministic trend (like a linear trend or a quadratic trend), then we can adjust for this by simply including these trends in our regression. To do this, we first create the trend variables (we will restrict ourselves to a linear trend and a quadratic trend), and then we simply add them to our regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear and quadratic trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control for trends in our main regression directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even controlling for trends renders the long-rate statistically insignificant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating stochastic (changing) trends (OPTIONAL)\n",
    "\n",
    "In some cases, non-stationarity could be caused by trends that change over time (e.g. a linear trend in the first part of the sample,  no trend in the middle, and a quadratic trend towards the end). In this case, the deterministic-trends approach from above may not accurately control for these trends and hence may not solve our non-stationarity problem.\n",
    "\n",
    "In this circumstance, it is more appropriate to estimate these stochastic trends first (for each series). Then subtract these trends from the series and use these de-trended variables in our regression instead. \n",
    "The **Hodrick-Prescott method** is very commonly used for this purpose and it outputs the detrended series directly (as well as the estimated trend). This method can be implemented using the ``.tsa.filters.hpfilter()`` function in the ``statsmodel`` package, as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate stochastic trend in short rate using Hodrick-Prescott method\n",
    "rates['r_3m_detrended'],rates['r_3m_trend'] = sm.tsa.filters.hpfilter(rates['r_3m_lead5'],lamb = 129600)\n",
    "        #IMPORTANT: we use this \"lamb\" value because we have monthly data. \n",
    "        #use lamb = 1600 for quarterly data and lamb = 6.25 for annual data\n",
    "        \n",
    "# Take a look at the short rate and its stochastic trend\n",
    "rates[['r_3m_lead5','r_3m_trend']].plot(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now take a look at the detrended short rate\n",
    "rates['r_3m_detrended'].plot(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate stochastic trend in long rate using Hodrick-Prescott method\n",
    "rates['r_10yr_detrended'],rates['r_10yr_trend'] = sm.tsa.filters.hpfilter(rates['r_10yr'],lamb = 129600)\n",
    "\n",
    "# Take a look at the results \n",
    "rates[['r_10yr','r_10yr_trend']].plot(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now take a look at the detrended long rate\n",
    "rates['r_10yr_detrended'].plot(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run regression using detrended series\n",
    "results = sm.OLS(rates['r_3m_detrended'], \n",
    "                 rates[['const','r_10yr_detrended']], missing='drop'\n",
    "                ).fit().get_robustcov_results(cov_type = 'HAC', maxlags = 5)\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, detrending the data using stochastic trends also renders the long rate insignificant. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
